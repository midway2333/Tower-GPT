# åŸºæœ¬åº“
import torch
import json
import numpy as np
import os, shutil
import math

# ç”¨äºå¯è§†åŒ–çš„åº“
from rich.progress import Progress, TextColumn, BarColumn, TimeRemainingColumn, TimeElapsedColumn
from torch.utils.tensorboard import SummaryWriter   # type: ignore

# tokenizer
import sentencepiece as spm
from transformers import AutoTokenizer

# ä¼˜åŒ–å™¨
from torch.optim.adamw import AdamW
from transformers import Adafactor
from galore_torch import GaLoreAdamW8bit
from galore_torch import GaLoreAdafactor

# æ¨¡å—åº“
from torch import GradScaler, autocast   # type: ignore
from torch.utils.data import DataLoader
from torch.nn import functional as fc
from torch import nn, Tensor

# å…¶ä»–åº“
from datetime import datetime
from typing import Optional, Union
import signal
import sys

# Dataset & DataLoader
from remaster.dataset import TextDataProcessor
from remaster.dataset import TextDataset, GeneratorTextDataset
from remaster.dataset import MultiTurn_DialogueDataProcessor
from remaster.dataset import Talk_DialogueDataset, Talk_GeneratorDialogueDataset

# æ¨¡å‹ç±»
from remaster.model import transformer

# é…ç½®æ–‡ä»¶
from remaster.config import TrainerConfig

# æ—¥å¿—ç±»
import logging
from remaster.logger import TrainLogger


class trainer():
    def __init__(self, config: TrainerConfig):
        """åˆå§‹åŒ–è®­ç»ƒå™¨

        å‚æ•°:
        - config (TrainerConfig): è®­ç»ƒé…ç½®
        """
        # æ¨¡å‹é…ç½®
        self.decoder_num = config.decoder_num
        """è§£ç å™¨æ•°é‡"""
        self.head_num = config.head_num
        """æ³¨æ„åŠ›å¤´æ•°"""
        self.d = config.d
        """éšè—å±‚ç»´åº¦"""
        self.dk = config.dk
        """KV ç»´åº¦"""
        self.dff = config.dff
        """å‰é¦ˆç½‘ç»œç»´åº¦"""
        self.vocab_size = config.vocab_size
        """è¯è¡¨å¤§å°"""

        # è®­ç»ƒå™¨é…ç½®
        self.train_method = config.train_method
        """è®­ç»ƒæ–¹å¼"""
        self.keep_train = config.keep_train
        """æ˜¯å¦ä»æœ€è¿‘æ£€æŸ¥ç‚¹ç»­è®­ç»ƒ"""
        self.ckpt_path = config.ckpt_path
        """æ£€æŸ¥ç‚¹è·¯å¾„"""
        self.finetune = config.finetune
        """æ˜¯å¦å¾®è°ƒæ¨¡å‹"""
        self.compile = config.compile
        """æ˜¯å¦ä½¿ç”¨ torch.compile ç¼–è¯‘æ¨¡å‹åŠ é€Ÿè®­ç»ƒ"""
        self.load_optimizer = config.load_optimizer
        """æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨"""

        # è®¾å¤‡é…ç½®
        self.device = config.device
        """åŠ è½½è®¾å¤‡"""
        self.mixed_precision = config.mixed_precision
        """æ··åˆç²¾åº¦è®­ç»ƒæ–¹å¼"""

        # æ¨¡å‹é…ç½®
        self.train_model_dir = config.train_model_dir
        """é¢„è®­ç»ƒæ¨¡å‹ç›®å½•"""
        self.train_model_name = config.train_model_name
        """é¢„è®­ç»ƒæ¨¡å‹å"""
        self.output_dir = config.output_dir
        """è¾“å‡ºç›®å½•"""
        self.output_model_name = config.output_model_name
        """è¾“å‡ºæ¨¡å‹å"""
        self.model_suffix = config.model_suffix
        """æ¨¡å‹æ–‡ä»¶åç¼€å"""
        self.optimizer_suffix = config.optimizer_suffix
        """ä¼˜åŒ–å™¨æ–‡ä»¶åç¼€å"""
        self.scheduler_suffix = config.scheduler_suffix
        """è°ƒåº¦å™¨æ–‡ä»¶åç¼€å"""
        self.max_checkpoints = config.max_checkpoints
        """æœ€å¤§ä¿å­˜æ£€æŸ¥ç‚¹æ•°é‡"""
        self.save_best_checkpoint = config.save_best_checkpoint
        """æ˜¯å¦ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹"""

        # æ•°æ®é›†é…ç½®
        self.train_data_path = config.train_data_path
        """è®­ç»ƒæ•°æ®è·¯å¾„"""
        self.valid_data_path = config.valid_data_path
        """éªŒè¯æ•°æ®è·¯å¾„"""
        self.test_data_path = config.test_data_path
        """æµ‹è¯•æ•°æ®è·¯å¾„"""
        self.tokenizer_path = config.tokenizer_path
        """åˆ†è¯å™¨è·¯å¾„"""
        self.num_workers = config.num_workers
        """æ•°æ®åŠ è½½å™¨å·¥ä½œçº¿ç¨‹æ•°"""
        self.pin_memory = config.pin_memory
        """æ˜¯å¦å°†æ•°æ®åŠ è½½åˆ° CUDA å›ºå®šå†…å­˜ä¸­"""
        self.yield_load = config.yield_load
        """æ˜¯å¦ä½¿ç”¨ yield åŠ è½½æ•°æ®"""

        # è®­ç»ƒå‚æ•°é…ç½®
        self.all_epochs = config.all_epochs
        """æ€»è®­ç»ƒè½®æ•°"""
        self.batch_size = config.batch_size
        """æ‰¹æ¬¡å¤§å°"""
        self.block_size = config.block_size
        """è¾“å…¥åºåˆ—é•¿åº¦"""
        self.accumulation_steps = config.accumulation_steps
        """æ¢¯åº¦ç´¯è®¡æ­¥æ•°"""
        self.info_update_interval = config.info_update_interval
        """ä¿¡æ¯æ›´æ–°é—´éš”"""

        # ä¼˜åŒ–å™¨é…ç½®
        self.optimizer_name = config.optimizer
        """ä¼˜åŒ–å™¨"""
        self.learning_rate = config.learning_rate
        """å­¦ä¹ ç‡"""
        self.betas = config.betas
        """beta å‚æ•°"""
        self.eps = config.eps
        """ä¼˜åŒ–å™¨ epsilon å‚æ•°"""
        self.weight_decay = config.weight_decay
        """æƒé‡è¡°å‡ç³»æ•°"""
        self.lr_scheduler = config.lr_scheduler
        """æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        self.pct_start = config.pct_start
        """å­¦ä¹ ç‡è°ƒåº¦å™¨çš„é¢„çƒ­æ¯”ä¾‹"""
        self.max_lr_rate = config.max_lr_rate
        """å­¦ä¹ ç‡è°ƒåº¦å™¨çš„æœ€å¤§å­¦ä¹ ç‡å€ç‡"""
        self.div_factor = config.div_factor
        """å­¦ä¹ ç‡è°ƒåº¦å™¨çš„åˆå§‹å­¦ä¹ ç‡å€ç‡"""
        self.anneal_strategy = config.anneal_strategy
        """å­¦ä¹ ç‡è°ƒåº¦å™¨çš„é€€ç«ç­–ç•¥"""

        # æ¨¡å‹æŠ€æœ¯å‚æ•°é…ç½®
        self.grad_clip = config.grad_clip
        """æ¢¯åº¦è£å‰ªå€¼"""
        self.grad_checkpoint = config.grad_checkpoint
        """æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯èŠ‚çœæ˜¾å­˜"""
        self.dropout = config.dropout
        """dropout æ¦‚ç‡"""

        # è¯„ä¼°é…ç½®
        self.ppl_eval = config.ppl_eval
        """æ˜¯å¦åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å›°æƒ‘åº¦"""
        self.bleu_eval = config.bleu_eval
        """æ˜¯å¦åœ¨éªŒè¯é›†ä¸Šè¯„ä¼° BLEU-4 åˆ†æ•°"""

        # å¯è§†åŒ–é…ç½®
        self.tensorboard = config.tensorboard
        """æ˜¯å¦ä½¿ç”¨ tensorboard å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"""
        self.tensorboard_dir = config.tensorboard_dir
        """tensorboard æ—¥å¿—ç›®å½•"""
        self.writer_name = config.writer_name
        """tensorboard æ—¥å¿—æ–‡ä»¶å"""

        # æ—¥å¿—é…ç½®
        self.logger_name = config.logger_name
        """æ—¥å¿—åç§°"""
        self.std_level = config.std_level
        """æ§åˆ¶å°è¾“å‡ºæ—¥å¿—çº§åˆ«"""
        self.file_level = config.file_level
        """æ–‡ä»¶è¾“å‡ºæ—¥å¿—çº§åˆ«"""
        self.std_out = config.std_out
        """æ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°"""
        self.save_info = config.save_info
        """æ˜¯å¦ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        self.file_name = config.file_name
        """æ—¥å¿—æ–‡ä»¶å"""

        self._build_logger()   # æ„å»ºæ—¥å¿—è®°å½•å™¨
        self._device_check()   # æ£€æŸ¥è®¾å¤‡ä¿¡æ¯

        self.best_blue: float = -float("inf")
        """æœ€ä½³ BLUE åˆ†æ•°"""
        self.best_ppl: float = float("inf")
        """æœ€ä½³ PPL åˆ†æ•°"""

        self.train_signal = False
        """ç”¨äºåˆ¤æ–­æ¨¡å‹æ˜¯å¦è¿›å…¥è®­ç»ƒæµç¨‹"""

        sp = spm.SentencePieceProcessor()
        self.tokenizer = sp.Load(self.tokenizer_path)
        """åˆ†è¯å™¨"""
        self.loss_fn = nn.CrossEntropyLoss(ignore_index=sp.pad_id())
        """æŸå¤±å‡½æ•°"""
        self.scaler = GradScaler() if self.mixed_precision != "full" else None
        """æ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾å™¨"""

        self.now_epoch = 0
        """å½“å‰è½®æ•°"""
        self.train_steps = 0
        """å½“å‰è®­ç»ƒæ­¥æ•°"""
        self.local_steps = 0
        """å½“å‰æ•°æ®åŠ è½½æ­¥æ•°"""
        self.info_steps = 0
        """å½“å‰ä¿¡æ¯æ›´æ–°æ­¥æ•°"""

        self._init_tensorboard()   # åˆå§‹åŒ– TensorBoard
        self._init_optimizer()     # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self._init_dataloader()    # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨



        signal.signal(signal.SIGINT, self.exit)

    def _build_logger(self):
        """æ„å»ºæ—¥å¿—è®°å½•å™¨"""
        assert self.std_level in ["debug", "info", "warning", "error"], f"æ—¥å¿—çº§åˆ«å¿…é¡»æ˜¯ 'debug', 'info', 'warning', æˆ– 'error', ä½†å¾—åˆ° {self.std_level}"
        assert self.file_level in ["debug", "info", "warning", "error"], f"æ—¥å¿—çº§åˆ«å¿…é¡»æ˜¯ 'debug', 'info', 'warning', æˆ– 'error', ä½†å¾—åˆ° {self.file_level}"

        # level = logging._nameToLevel[self.level.upper()]
        LEVEL_MAP = {
            "debug": logging.DEBUG,
            "info": logging.INFO,
            "warning": logging.WARNING,
            "error": logging.ERROR,
        }
        std_level = LEVEL_MAP[self.std_level]
        file_level = LEVEL_MAP[self.file_level]
        # æ˜ å°„ log çº§åˆ«

        self.logger = TrainLogger(
            logger_name=self.logger_name,
            std_level=std_level,
            file_level=file_level,
            std_out=self.std_out,
            save_info=self.save_info,
            output_dir=self.output_dir,
            file_name=self.file_name,
        )   # æ„å»ºæ—¥å¿—è®°å½•å™¨

    def _device_check(self):
        """æ£€æŸ¥è®¾å¤‡ä¿¡æ¯"""
        assert self.device in ["cpu", "cuda", "xpu", "mps","auto"], f"è®¾å¤‡å¿…é¡»æ˜¯ 'cpu', 'cuda', 'xpu', 'mps' æˆ– 'auto', ä½†å¾—åˆ° {self.device}"
        # æ–­è¨€è®¾å¤‡ä¿¡æ¯æ— è¯¯

        if self.device == "auto":   # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
            if torch.cuda.is_available():
                self.device = "cuda"
                self.logger.info("AUTO: æˆåŠŸåŠ è½½ Nvidia CUDA")

            elif torch.xpu.is_available():
                self.device = "xpu"
                self.logger.info("AUTO: æˆåŠŸåŠ è½½ Intel XPU")

            elif torch.backends.mps.is_available():
                self.device = "mps"
                self.logger.info("AUTO: æˆåŠŸåŠ è½½ Apple MPS")

            else:
                if torch.cpu.is_available():
                    self.device = "cpu"
                    self.logger.info("AUTO: æˆåŠŸåŠ è½½ CPU")
                    self.logger.warning("AUTO: æœªæ£€æµ‹åˆ°ä»»ä½•å¯è¡Œçš„åŠ é€Ÿæ–¹å¼, è‡ªåŠ¨é€‰æ‹© CPU; è®­ç»ƒå¯èƒ½å—å½±å“")

                else:
                    raise RuntimeError("AUTO: æœªæ£€æµ‹åˆ°å¯ç”¨è®¾å¤‡; è¯·æ£€æŸ¥è®¾å¤‡é…ç½®")

        if self.device == "cuda":   # æ‰‹åŠ¨é€‰æ‹© CUDA
            if torch.cuda.is_available():
                self.logger.info(f"Manual: æˆåŠŸåŠ è½½ Nvidia CUDA")

            else:
                self.logger.error("Manual: Nvidia CUDA åŠ è½½å¤±è´¥; è¯·æ£€æŸ¥è®¾å¤‡")
                raise RuntimeError("Nvidia CUDA åŠ è½½å¤±è´¥; è¯·æ£€æŸ¥è®¾å¤‡")
            
        if self.device == "xpu":   # æ‰‹åŠ¨é€‰æ‹© XPU
            if torch.xpu.is_available():
                self.logger.info(f"Manual: æˆåŠŸåŠ è½½ Intel XPU")

            else:
                self.logger.error("Manual: Intel XPU åŠ è½½å¤±è´¥; è¯·æ£€æŸ¥è®¾å¤‡")
                raise RuntimeError("Intel XPU åŠ è½½å¤±è´¥; è¯·æ£€æŸ¥è®¾å¤‡")
            
        if self.device == "mps":   # æ‰‹åŠ¨é€‰æ‹© MPS
            if torch.backends.mps.is_available():
                self.logger.info(f"Manual: æˆåŠŸåŠ è½½ Apple MPS")

            else:
                self.logger.error("Manual: Apple MPS åŠ è½½å¤±è´¥; è¯·æ£€æŸ¥è®¾å¤‡")
                raise RuntimeError("Apple MPS åŠ è½½å¤±è´¥; è¯·æ£€æŸ¥è®¾å¤‡")
            
        if self.device == "cpu":   # æ‰‹åŠ¨é€‰æ‹© CPU
            if torch.cpu.is_available():
                self.logger.info(f"Manual: æˆåŠŸåŠ è½½ CPU")
                self.logger.warning("Manual: æ‰‹åŠ¨é€‰æ‹© CPU; è®­ç»ƒå¯èƒ½å—å½±å“")

            else:
                self.logger.error("Manual: CPU åŠ è½½å¤±è´¥; è¯·æ£€æŸ¥è®¾å¤‡")
                raise RuntimeError("CPU åŠ è½½å¤±è´¥; è¯·æ£€æŸ¥è®¾å¤‡")
            # å°½ç®¡ torch.cpu.is_available() åœ¨ä¸€èˆ¬æƒ…å†µä¸‹æ’ä¸º True
            # æ­¤å¤„ä¸ºä¿æŒè®¾å¤‡æ£€æŸ¥é€»è¾‘ç»Ÿä¸€ä»ä½¿ç”¨

    def _init_dataloader(self):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
        assert self.train_method in ["text", "chat"], f"è®­ç»ƒæ–¹æ³•å¿…é¡»æ˜¯ 'text' æˆ– 'chat', ä½†å¾—åˆ° {self.train_method}"
        # æ–­è¨€è®­ç»ƒæ–¹æ³•ä¿¡æ¯æ— è¯¯

        if self.train_method == "text":
            self.processor = TextDataProcessor(
                json_file=self.train_data_path,
                sp_model_path=self.tokenizer_path,
                block_size=self.block_size,
            )

        elif self.train_method == "chat":
            self.processor = MultiTurn_DialogueDataProcessor(
                json_file=self.train_data_path,
                sp_model_path=self.tokenizer_path,
                block_size=self.block_size,
            )

        if self.yield_load:
            if self.train_method == "text":
                self.train_dataset = GeneratorTextDataset(self.processor)   # type: ignore
            elif self.train_method == "chat":
                self.train_dataset = Talk_DialogueDataset(self.processor)   # type: ignore

            self.train_dataloader = DataLoader(
                self.train_dataset,
                batch_size=self.batch_size,
                shuffle=True,
                num_workers=0,   # yield ä¸‹å¿…é¡»ä¸º 0
            )   # åˆå§‹åŒ–è®­ç»ƒæ•°æ®åŠ è½½å™¨

        else:
            if self.train_method == "text":
                self.train_dataset = TextDataset(self.processor)   # type: ignore
            elif self.train_method == "chat":
                self.train_dataset = Talk_DialogueDataset(self.processor)   # type: ignore

            self.train_dataloader = DataLoader(
                self.train_dataset,
                batch_size=self.batch_size,
                shuffle=True,
                num_workers=self.num_workers,
                pin_memory=self.pin_memory,
            )   # åˆå§‹åŒ–è®­ç»ƒæ•°æ®åŠ è½½å™¨

        if self.valid_data_path:   # åˆå§‹åŒ–éªŒè¯æ•°æ®åŠ è½½å™¨
            if self.train_method == "text":
                self.valid_dataset = TextDataset(self.processor)   # type: ignore
            elif self.train_method == "chat":
                self.valid_dataset = Talk_DialogueDataset(self.processor)   # type: ignore

            self.valid_dataloader = DataLoader(
                self.valid_dataset,
                batch_size=self.batch_size,
                shuffle=False,
                num_workers=self.num_workers,
                pin_memory=self.pin_memory,
            )   # åˆå§‹åŒ–éªŒè¯æ•°æ®åŠ è½½å™¨

        else:
            self.valid_dataloader = None

        if self.test_data_path:   # åˆå§‹åŒ–æµ‹è¯•æ•°æ®åŠ è½½å™¨
            if self.train_method == "text":
                self.test_dataset = TextDataset(self.processor)   # type: ignore
            elif self.train_method == "chat":
                self.test_dataset = Talk_DialogueDataset(self.processor)   # type: ignore

            self.test_dataloader = DataLoader(
                self.test_dataset,
                batch_size=self.batch_size,
                shuffle=False,
                num_workers=self.num_workers,
                pin_memory=self.pin_memory,
            )   # åˆå§‹åŒ–éªŒè¯æ•°æ®åŠ è½½å™¨

        else:
            self.test_dataloader = None

        self.data_length = self.processor.data_length()   # è®­ç»ƒæ•°æ®é›†é•¿åº¦

    def _init_optimizer(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        OPTIMIZER_MAP = {
            "adamw": AdamW,
            "adafactor": Adafactor,
            "galore_adamw": GaLoreAdamW8bit,
            "galore_adafactor": GaLoreAdafactor,
        }

        assert self.optimizer_name in OPTIMIZER_MAP, f"ä¼˜åŒ–å™¨å¿…é¡»æ˜¯ {list(OPTIMIZER_MAP.keys())}, ä½†å¾—åˆ° {self.optimizer_name}"
        # æ–­è¨€ä¼˜åŒ–å™¨ä¿¡æ¯æ— è¯¯

        self.optimizer: AdamW | GaLoreAdamW8bit | Adafactor | GaLoreAdafactor
        if self.optimizer_name in ["adamw", "galore_adamw"]:
            self.optimizer = OPTIMIZER_MAP[self.optimizer_name](
                self.model.parameters(),
                lr=self.learning_rate,
                betas=self.betas,
                eps=self.eps,
                weight_decay=self.weight_decay,
            )   # åˆå§‹åŒ– AdamWç³» ä¼˜åŒ–å™¨

        else:
            self.optimizer = OPTIMIZER_MAP[self.optimizer_name](
                self.model.parameters(),
                lr=self.learning_rate,
                beta1=self.betas,
                eps=self.eps,
                weight_decay=self.weight_decay,
            )   # åˆå§‹åŒ– Adafactorç³» ä¼˜åŒ–å™¨

        self.logger.debug(f"ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ: {self.optimizer}")

    def _init_lr_scheduler(self):
        """åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if not self.lr_scheduler:
            self.rate_scheduler = None
            return

        assert self.max_lr_rate is not None, "max_lr_rate å¿…é¡»æŒ‡å®š"
        assert self.div_factor is not None, "div_factor å¿…é¡»æŒ‡å®š"
        assert self.pct_start is not None, "pct_start å¿…é¡»æŒ‡å®š"
        assert self.anneal_strategy in ["linear", "cos"], f"anneal_strategy å¿…é¡»æŒ‡å®šä¸º linear æˆ– cos, ä½†å¾—åˆ° {self.anneal_strategy}"
        last_step = -1 if self.train_steps == 0 else self.train_steps - 1
        self.rate_scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer=self.optimizer,
            max_lr=self.max_lr_rate * self.learning_rate,
            epochs=self.all_epochs,
            cycle_momentum=False,
            steps_per_epoch=int(np.ceil(self.data_length / (self.batch_size * self.accumulation_steps))),
            div_factor=self.div_factor,
            last_epoch=last_step,
            pct_start=self.pct_start,
            anneal_strategy=self.anneal_strategy,   # type: ignore
        )   # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨


    def _init_tensorboard(self):
        """åˆå§‹åŒ– TensorBoard"""
        if self.tensorboard:
            self.writer = SummaryWriter(self.tensorboard_dir)
            # åˆå§‹åŒ– TensorBoard å†™å…¥å™¨

        else:
            self.writer = None

    def _save_resume_page(self, dir: str, suffix: str | int, now_time: str):
        """ä¿å­˜æ¢å¤é¡µé¢
        
        å‚æ•°:
        - dir (str): ä¿å­˜ç›®å½•
        - suffix (str | int): ä¿å­˜åç¼€
        - now_time (str): å½“å‰æ—¶é—´
        """
        os.makedirs(os.path.join(self.output_dir, dir), exist_ok=True)
        # ç¡®ä¿ç›®å½•å­˜åœ¨

        save_path = os.path.join(
            self.output_dir,
            dir,
            f"{self.output_model_name}_{suffix}.log",
        )   # æ„å»ºä¿å­˜è·¯å¾„

        resume_page = {
            "model_file": f"{self.output_model_name}_{suffix}.{self.model_suffix}",
            "optimizer_file": f"{self.output_model_name}_{suffix}.{self.optimizer_suffix}",
            "scheduler_file": f"{self.output_model_name}_{suffix}.{self.scheduler_suffix}" if self.scheduler_suffix else None,
            "train_data_path": self.train_data_path,
            "valid_data_path": self.valid_data_path,
            "test_data_path": self.test_data_path,
            "tokenizer_path": self.tokenizer_path,
            "train_method": self.train_method,
            "finetune": self.finetune,
            "batch_size": self.batch_size,
            "accumulation_steps": self.accumulation_steps,
            "block_size": self.block_size,
            "time": now_time,
            "all_epochs": self.all_epochs,
            "now_epoch": self.now_epoch,
            "train_steps": self.train_steps,
            "tensorboard": self.tensorboard,
            "tensorboard_dir": self.tensorboard_dir,
            "writer_name": self.writer_name,
            "skip_steps": self.local_steps,   # å·²ç»åŠ è½½å¤šå°‘, å°±è·³è¿‡å¤šå°‘
        }   # æ¢å¤é¡µé¢å†…å®¹

        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(resume_page, f, ensure_ascii=False, indent=4)
        # ä¿å­˜æ¢å¤é¡µé¢

        self.logger.info(f"æ¢å¤é¡µé¢å·²ä¿å­˜: {save_path}")   # logger

    def _load_resume_page(self):
        """åŠ è½½æ¢å¤é¡µé¢"""
        if self.ckpt_path is None:
            self.logger.warning("æ£€æŸ¥ç‚¹è·¯å¾„ä¸ºç©º, æ— æ³•åŠ è½½æ¢å¤é¡µé¢")
            raise RuntimeError("æ£€æŸ¥ç‚¹è·¯å¾„ä¸ºç©º, æ— æ³•åŠ è½½æ¢å¤é¡µé¢")
        # æ£€æŸ¥ç‚¹è·¯å¾„ä¸ºç©º, ç›´æ¥è¿”å› None

        try:
            with open(self.ckpt_path, "r", encoding="utf-8") as f:
                resume_page = json.load(f)
            # åŠ è½½æ¢å¤é¡µé¢

            self.resume_model_path = os.path.join(os.path.dirname(self.ckpt_path), resume_page["model_file"])
            # ä»æ¢å¤é¡µé¢ä¸­æå–æ¨¡å‹å­è·¯å¾„

            self.resume_optimizer_path = os.path.join(os.path.dirname(self.ckpt_path), resume_page["optimizer_file"])
            # ä»æ¢å¤é¡µé¢ä¸­æå–ä¼˜åŒ–å™¨å­è·¯å¾„

            self.resume_scheduler_path = os.path.join(os.path.dirname(self.ckpt_path), resume_page["scheduler_file"]) if resume_page["scheduler_file"] else None
            # ä»æ¢å¤é¡µé¢ä¸­æå–è°ƒåº¦å™¨å­è·¯å¾„, è‹¥ä¸å­˜åœ¨åˆ™ä¸º None

            self.now_epoch = resume_page["now_epoch"]
            # ä»æ¢å¤é¡µé¢ä¸­æå–å½“å‰ epoch

            self.train_steps = resume_page["train_steps"]
            # ä»æ¢å¤é¡µé¢ä¸­æå–è®­ç»ƒæ­¥æ•°

            self.skip_steps = resume_page["skip_steps"]
            # ä»æ¢å¤é¡µé¢ä¸­æå–å·²ç»åŠ è½½çš„æ­¥æ•°, è·³è¿‡å¤šå°‘æ­¥

            self.logger.info(f"æ¢å¤é¡µé¢å·²åŠ è½½: {self.ckpt_path}")   # logger

        except Exception as e:
            self.logger.error(f"åŠ è½½æ¢å¤é¡µé¢æ—¶å‡ºé”™: {e}")
            raise RuntimeError(f"åŠ è½½æ¢å¤é¡µé¢æ—¶å‡ºé”™: {e}")

    def _forward_calc(
        self,
        inputs: Tensor,
        target: Tensor,
        loss_mask: Tensor | None = None,
        accumulation_steps: int = 1,
    ) -> Tensor:
        """å‰å‘è®¡ç®—

        å‚æ•°:
        - inputs (Tensor): è¾“å…¥å¼ é‡, å½¢çŠ¶ä¸º (batch_size, seq_len)
        - target (Tensor): ç›®æ ‡å¼ é‡, å½¢çŠ¶ä¸º (batch_size, seq_len)
        - loss_mask (Tensor | None): æŸå¤±æ©ç å¼ é‡, å½¢çŠ¶ä¸º (batch_size, seq_len), é»˜è®¤ä¸º None
        - accumulation_steps (int): æ¢¯åº¦ç´¯ç§¯æ­¥æ•°, é»˜è®¤ä¸º 1

        è¿”å›:
        - Tensor: æŸå¤±å¼ é‡, å½¢çŠ¶ä¸º [loss]
        """
        inputs = inputs.to(self.device)
        target = target.to(self.device)
        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡

        pred: Tensor = self.model(inputs)   # å‰å‘è®¡ç®—
        pred = pred.view(-1, pred.size(-1))
        # å°† pred é‡æ–°å½¢çŠ¶ä¸º [batch_size * sequence_length, vocab_size]

        target = target.view(-1)
        # å°† target é‡æ–°å½¢çŠ¶ä¸º [batch_size * sequence_length]

        all_loss: Tensor = self.loss_fn(pred, target)
        # è®¡ç®—æ‰€æœ‰ä½ç½®çš„loss

        if loss_mask is not None:
            loss_mask = loss_mask.to(self.device)
            # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡

            loss_mask = loss_mask.view(-1)   # å±•å¹³æ©ç 
            masked_loss = (all_loss * loss_mask) / accumulation_steps
            loss = masked_loss.sum() / (loss_mask.sum() + 1e-8)   # é¿å…é™¤é›¶
            # åªä¿ç•™åŠ©æ‰‹å›å¤éƒ¨åˆ†çš„loss

        else:
            loss = all_loss.mean() / accumulation_steps   # è®¡ç®—å¹³å‡æŸå¤±

        return loss

    def _mixed_dtype(self) -> torch.dtype:
        """æ ¹æ®æ··åˆç²¾åº¦æ¨¡å¼è¿”å›å¯¹åº” dtype

        å‚æ•°:
        - dtype (str): æ··åˆç²¾åº¦æ¨¡å¼, å¯é€‰å€¼ä¸º "full" "fp16" "bf16"

        è¿”å›:
        - torch.dtype: å¯¹åº” dtype
        """
        if self.mixed_precision == "full":
            return torch.float32
        elif self.mixed_precision == "fp16":
            return torch.float16
        elif self.mixed_precision == "bf16":
            return torch.bfloat16
        else:
            return torch.float16

    def _load_from_base_model(self):
        """ä»åŸºç¡€æ¨¡å‹åŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€"""
        try:
            assert self.train_model_dir is not None and self.train_model_name is not None, "è®­ç»ƒæ¨¡å‹ç›®å½•æˆ–åç§°ä¸ºç©º, æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹"
            model_path = os.path.join(self.train_model_dir, self.train_model_name+self.model_suffix)
            optimizer_path = os.path.join(self.train_model_dir, self.train_model_name+self.optimizer_suffix)

            self.model.load_state_dict(torch.load(model_path, map_location=self.device))
            self.logger.debug(f"æˆåŠŸåŠ è½½åŸºç¡€æ¨¡å‹æ¨¡å‹: {model_path}")
            # åŠ è½½æ¨¡å‹çŠ¶æ€

            try:
                self.optimizer.load_state_dict(torch.load(optimizer_path, map_location=self.device))
                self.logger.debug(f"æˆåŠŸåŠ è½½åŸºç¡€æ¨¡å‹ä¼˜åŒ–å™¨: {optimizer_path}")
            except Exception as e:
                self.logger.warning(f"åŠ è½½åŸºç¡€æ¨¡å‹ä¼˜åŒ–å™¨å¤±è´¥, å¯èƒ½å½±å“è®­ç»ƒ")
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€

            if self.scheduler_suffix and self.rate_scheduler is not None:
                try:
                    scheduler_path = os.path.join(self.train_model_dir, self.train_model_name+self.scheduler_suffix)
                    self.rate_scheduler.load_state_dict(torch.load(scheduler_path, map_location=self.device))
                    self.logger.debug(f"æˆåŠŸåŠ è½½åŸºç¡€æ¨¡å‹è°ƒåº¦å™¨: {scheduler_path}")
                except Exception as e:
                    self.logger.warning(f"åŠ è½½åŸºç¡€æ¨¡å‹è°ƒåº¦å™¨å¤±è´¥, å¯èƒ½å½±å“è®­ç»ƒ")
                # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€

            self.logger.info(f"æˆåŠŸä»åŸºç¡€æ¨¡å‹åŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€: {model_path}")
            # logger
        
        except Exception as e:
            self.logger.error(f"ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {e}")
            raise RuntimeError(f"ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {e}")

    def _load_from_resume(self):
        """ä»æ¢å¤é¡µåŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€"""
        try:
            self.model.load_state_dict(torch.load(self.resume_model_path, map_location=self.device))
            self.logger.debug(f"æˆåŠŸåŠ è½½æ¢å¤æ¨¡å‹: {self.resume_model_path}")
            # åŠ è½½æ¨¡å‹çŠ¶æ€

            self.optimizer.load_state_dict(torch.load(self.resume_optimizer_path, map_location=self.device))
            self.logger.debug(f"æˆåŠŸåŠ è½½æ¢å¤ä¼˜åŒ–å™¨: {self.resume_optimizer_path}")
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€

            if self.resume_scheduler_path is not None and self.rate_scheduler is not None:
                self.rate_scheduler.load_state_dict(torch.load(self.resume_scheduler_path, map_location=self.device))
                self.logger.debug(f"æˆåŠŸåŠ è½½ä¼˜åŒ–è°ƒåº¦å™¨: {self.resume_scheduler_path}")
            # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€

            self.logger.info(f"æˆåŠŸä»æ¢å¤é¡µåŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€: {self.ckpt_path}")
            # logger

        except Exception as e:
            self.logger.error(f"ä»æ¢å¤é¡µåŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {e}")
            raise RuntimeError(f"ä»æ¢å¤é¡µåŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {e}")

    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        self.model = transformer(
            decoder_num=self.decoder_num,
            head_num=self.head_num,
            d=self.d,
            dk=self.dk,
            dff=self.dff,
            vocab_size=self.vocab_size,
            dropout=self.dropout,
        )   # æ„å»ºæ¨¡å‹
        self.logger.debug("æ¨¡å‹æ„å»ºå®Œæˆ")

        if self.compile:   # ç¼–è¯‘æ¨¡å‹
            try:
                self.model = torch.compile(self.model)
                self.logger.debug("æ¨¡å‹ç¼–è¯‘å®Œæˆ")

            except Exception as e:
                self.logger.error(f"æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
                raise RuntimeError(f"æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
            
        self.model.to(self.device)   # æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.logger.debug(f"æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.device}")

        self._init_optimizer()   # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.logger.debug("ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")

        if self.keep_train and self.train_model_dir and self.train_model_name is not None:
            self.logger.debug("åŠ è½½æ£€æŸ¥ç‚¹")
            try:
                train_path = os.path.join(
                    self.train_model_dir,
                    self.train_model_name,
                    self.model_suffix,
                )   # æ„å»ºæ£€æŸ¥ç‚¹è·¯å¾„

                self.model.load_state_dict(torch.load(train_path, map_location=self.device))
                self.logger.debug(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹: {train_path}")
                # åŠ è½½æ¨¡å‹çŠ¶æ€

                if self.load_optimizer:   # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
                    try:
                        optimizer_path = os.path.join(
                            self.train_model_dir,
                            self.train_model_name,
                            self.optimizer_suffix,
                        )   # æ„å»ºä¼˜åŒ–å™¨æ£€æŸ¥ç‚¹è·¯å¾„

                        self.optimizer.load_state_dict(torch.load(optimizer_path, map_location=self.device))
                        self.logger.debug(f"æˆåŠŸåŠ è½½ä¼˜åŒ–å™¨æ£€æŸ¥ç‚¹: {optimizer_path}")

                    except Exception as e:
                        self.logger.error(f"åŠ è½½ä¼˜åŒ–å™¨æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                        raise RuntimeError(f"åŠ è½½ä¼˜åŒ–å™¨æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

            except Exception as e:
                self.logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                raise RuntimeError(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def save_checkpoint(self, dir: str, suffix: str | int):
        """ä¿å­˜æ£€æŸ¥ç‚¹
        
        å‚æ•°:
        - dir: æ£€æŸ¥ç‚¹ç›®å½•
        - suffix: æ£€æŸ¥ç‚¹åç¼€, å¦‚ "epoch_1" "best"
        """
        os.makedirs(os.path.join(self.output_dir, dir), exist_ok=True)
        # ç¡®ä¿ç›®å½•å­˜åœ¨

        save_path = os.path.join(
            self.output_dir,
            dir,
            f"{self.output_model_name}_{suffix}",
            self.model_suffix,
        )   # æ„å»ºä¿å­˜è·¯å¾„

        save_optimizer_path = os.path.join(
            self.output_dir,
            dir,
            f"{self.output_model_name}_{suffix}",
            self.optimizer_suffix,
        )   # æ„å»ºä¼˜åŒ–å™¨ä¿å­˜è·¯å¾„

        if self.lr_scheduler is not None and self.scheduler_suffix is not None:
            save_scheduler_path = os.path.join(
                self.output_dir,
                dir,
                f"{self.output_model_name}_{suffix}",
                self.scheduler_suffix,
            )   # æ„å»ºè°ƒåº¦å™¨ä¿å­˜è·¯å¾„

        os.makedirs(os.path.dirname(save_path), exist_ok=True)         # ç¡®ä¿ç›®å½•å­˜åœ¨
        torch.save(self.model.state_dict(), save_path)                 # ä¿å­˜æ¨¡å‹çŠ¶æ€
        torch.save(self.optimizer.state_dict(), save_optimizer_path)   # ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€

        if self.lr_scheduler is not None and self.rate_scheduler is not None:
            torch.save(
                self.rate_scheduler.state_dict(),
                save_scheduler_path,
            )   # ä¿å­˜è°ƒåº¦å™¨çŠ¶æ€

        self.logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")   # logger

    def check_best_checkpoint(self, new_val_blue: float | None, new_val_loss: float, update_step: int):
        """æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹, BLUE ä¼˜å…ˆ
        
        å‚æ•°:
        - new_blue: æ–°çš„ BLUE åˆ†æ•°
        - new_ppl: æ–°çš„ PPL åˆ†æ•°
        - update_step: å½“å‰è®­ç»ƒæ›´æ–°æ­¥æ•°
        """
        if new_val_blue is not None:
            if new_val_blue > self.best_blue:   # BLUE ä¼˜å…ˆ
                self.best_blue = new_val_blue
                self.best_loss = new_val_loss
                self.save_checkpoint("best_checkpoint", "best_blue")
                self.logger.info(f"[BEST BLUE] æ–°æœ€ä½³ BLUE åˆ†æ•°: {new_val_blue:.4f}, Loss: {new_val_loss:.4f}, æ›´æ–°æ­¥æ•°: {update_step}")

            if new_val_loss < self.best_loss:   # Loss è®¡ç®—
                self.best_loss = new_val_loss
                self.save_checkpoint("best_checkpoint", "best_loss")
                self.logger.info(f"[BEST LOSS] æ–°æœ€ä½³ BLUE åˆ†æ•°: {new_val_blue:.4f}, Loss: {new_val_loss:.4f}, æ›´æ–°æ­¥æ•°: {update_step}")

        else:
            if new_val_loss < self.best_loss:   # Loss è®¡ç®—
                self.best_loss = new_val_loss
                self.save_checkpoint("best_checkpoint", "best_loss")
                self.logger.info(f"[BEST LOSS] æ–°æœ€ä½³ Loss åˆ†æ•°: {new_val_loss:.4f}, æ›´æ–°æ­¥æ•°: {update_step}, æ—  BLUE åˆ†æ•°")

    def delete_checkpoint(self, dir: str="time"):
        """åˆ é™¤æ—§çš„æ—¶é—´é¡ºåºæ£€æŸ¥ç‚¹
        
        å‚æ•°:
        - dir: æ£€æŸ¥ç‚¹ç›®å½•, é»˜è®¤ "time"
        """
        try:
            delete_dir = os.path.join(self.output_dir, dir)
            # æ„å»ºè¦åˆ é™¤çš„ç›®å½•

            if not os.path.exists(delete_dir):   # å¦‚æœç›®å½•ä¸å­˜åœ¨, ç›´æ¥è¿”å›
                self.logger.debug(f"ç›®å½•ä¸å­˜åœ¨, æ— éœ€åˆ é™¤æ£€æŸ¥ç‚¹: {delete_dir}")
                return

            checkpoints = []
            for item in os.listdir(delete_dir):   # åˆ—å‡ºæ–‡ä»¶
                item_path = os.path.join(delete_dir, item)   # æ„å»ºå­æ–‡ä»¶è·¯å¾„

                if os.path.isdir(item_path):   # è·å–è¯¥å­æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´
                    mtime = os.path.getmtime(item_path)
                    checkpoints.append((mtime, item, item_path))   # æ·»åŠ  (æœ€åä¿®æ”¹æ—¶é—´, ç›®å½•å, å®Œæ•´è·¯å¾„) åˆ°åˆ—è¡¨

            checkpoints.sort(key=lambda x: x[0])   # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº (æœ€æ—§çš„åœ¨å‰)
            if len(checkpoints) > self.max_checkpoints:
                for i in range(len(checkpoints) - self.max_checkpoints):
                    mtime, checkpoint_name, checkpoint_path = checkpoints[i]
                    shutil.rmtree(checkpoint_path)   # åˆ é™¤ç›®å½•
                    self.logger.debug(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {checkpoint_path}")

        except Exception as e:
            self.logger.error(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def train(self):
        """è®­ç»ƒä¸»è¿›ç¨‹"""

    def train_one_epoch(self):
        """è®­ç»ƒä¸€ä¸ªè½®æ¬¡"""
        total_loss = 0.0
        for step, (x, y, loss_mask) in enumerate(self.train_dataloader):   # ç”Ÿæˆæ­¥è¿›ç´¢å¼•
            x: Tensor = x.to(self.device).long()
            y: Tensor = y.to(self.device).long() 
            loss_mask: Tensor | None = loss_mask.to(self.device).float() if loss_mask is not None else None
            # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ä¸Š

            with autocast(device_type=str(self.device), dtype=self._mixed_dtype()):
                loss = self._forward_calc(x, y, loss_mask, self.accumulation_steps)
            # å‰å‘è®¡ç®—

            if self.scaler is not None:   # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
            else:
                loss.backward()

            total_loss += loss.item()
            self.local_steps = step
            # ç´¯è®¡æŸå¤±å’Œæœ¬åœ°æ­¥æ•°æ›´æ–°

            self.train_progress.update(self.tsp_progress, advance=1/self.info_update_interval)
            # æ›´æ–°è®­ç»ƒè¿›åº¦æ¡

            if (step + 1) % self.accumulation_steps == 0:   # æ¢¯åº¦æ›´æ–°
                if self.grad_clip is not None:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_clip)
                    # æ¢¯åº¦è£å‰ª

                if self.scaler is not None:   # æ··åˆç²¾åº¦æ›´æ–°å‚æ•°
                    self.scaler.step(self.optimizer)
                    self.scaler.update()

                else:   # å¸¸è§„æ›´æ–°å‚æ•°
                    self.optimizer.step()

                self.optimizer.zero_grad(set_to_none=True)
                self.train_steps += 1
                # æ¢¯åº¦æ¸…ç©º

                if self.rate_scheduler is not None:
                    self.rate_scheduler.step()
                    # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨

            if (step + 1) % self.info_update_interval == 0:
                self.info_steps += 1

                tsp_show_txt = 'train_info_steps: {}/{}'.format(
                        self.info_steps, self.all_tsp
                )   # è®¾ç½®tspæ›´æ–°ä¿¡æ¯

                self.train_progress.update(self.tsp_progress, show_info=tsp_show_txt)
                # æ›´æ–° tsp ä¿¡æ¯

                avg_loss = (total_loss / self.info_update_interval) * self.accumulation_steps
                # è®¡ç®—å¹³å‡æŸå¤±

                total_loss = 0.0   # é‡ç½®æ€»æŸå¤±

                if self.writer is not None and self.writer_name is not None:
                    self.writer.add_scalar(self.writer_name+'_train_loss', avg_loss, self.info_steps)
                    # è®°å½•è®­ç»ƒæŸå¤±

                    if self.ppl_eval:
                        ppl = math.exp(avg_loss)
                        self.writer.add_scalar(self.writer_name+'_train_ppl', ppl, self.info_steps)
                        # è®°å½•è®­ç»ƒ PPL

                self.evaluate()
                # è¯„ä¼°æ¨¡å‹

                self.save_checkpoint(self.output_dir, f"epoch_{self.now_epoch}_step_{self.train_steps}")
                # ä¿å­˜æ£€æŸ¥ç‚¹

    def evaluate(self):
        pass

    def test(self):
        """è¿ç”¨è¯„ä¼°é›†æµ‹è¯•æ¨¡å‹"""
        if self.test_dataloader is not None:
            self.model.eval()   # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            epoch_test_loss = 0

            with autocast(device_type=str(self.device), dtype=self._mixed_dtype()):
            # è‡ªåŠ¨æ··åˆç²¾åº¦

                with torch.no_grad():  # ä¸éœ€è¦è®¡ç®—æ¢¯åº¦
                    for tx, ty, t_mask in self.test_dataloader:
                        test_loss = self._forward_calc(tx, ty, t_mask)
                        epoch_test_loss += test_loss.item()
                        # åŒä¸Š
                
            self.avg_test_loss = (epoch_test_loss / len(self.test_dataloader))   # type: ignore
            # è®¡ç®—æŸå¤±

            if self.writer is not None and self.writer_name is not None:   # è®°å½•æµ‹è¯•æŸå¤±
                self.writer.add_scalar(self.writer_name+'_test', self.avg_test_loss, self.train_steps)

            self.model.train()

        else:   # æ— æµ‹è¯•é›†æ—¶è·³è¿‡
            pass

    def print_info(self):
        """æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯"""
        print("=" * 60)
        print("è®­ç»ƒé…ç½®ä¿¡æ¯")
        print("=" * 60)
        
        # æ¨¡å‹é…ç½®
        print("ğŸ“Š æ¨¡å‹é…ç½®:")
        print(f"  â”œâ”€â”€ è§£ç å™¨å±‚æ•°: {self.decoder_num}")
        print(f"  â”œâ”€â”€ æ³¨æ„åŠ›å¤´æ•°: {self.head_num}")
        print(f"  â”œâ”€â”€ éšè—å±‚ç»´åº¦: {self.d}")
        print(f"  â”œâ”€â”€ KV ç»´åº¦: {self.dk}")
        print(f"  â”œâ”€â”€ å‰é¦ˆç½‘ç»œç»´åº¦: {self.dff}")
        print(f"  â””â”€â”€ è¯è¡¨å¤§å°: {self.vocab_size}")
        
        # è®­ç»ƒå™¨é…ç½®
        print("ğŸš€ è®­ç»ƒå™¨é…ç½®:")
        print(f"  â”œâ”€â”€ è®­ç»ƒæ–¹å¼: {self.train_method}")
        print(f"  â”œâ”€â”€ ç»­è®­ç»ƒ: {self.keep_train}")
        print(f"  â”œâ”€â”€ å¾®è°ƒæ¨¡å¼: {self.finetune}")
        print(f"  â”œâ”€â”€ ç¼–è¯‘ä¼˜åŒ–: {self.compile}")
        print(f"  â””â”€â”€ åŠ è½½ä¼˜åŒ–å™¨: {self.load_optimizer}")
        
        # è®¾å¤‡é…ç½®
        print("ğŸ’» è®¾å¤‡é…ç½®:")
        print(f"  â”œâ”€â”€ è®­ç»ƒè®¾å¤‡: {self.device}")
        print(f"  â””â”€â”€ æ··åˆç²¾åº¦: {self.mixed_precision}")
        
        # è·¯å¾„é…ç½®
        print("ğŸ“ è·¯å¾„é…ç½®:")
        print(f"  â”œâ”€â”€ é¢„è®­ç»ƒæ¨¡å‹ç›®å½•: {self.train_model_dir}")
        print(f"  â”œâ”€â”€ é¢„è®­ç»ƒæ¨¡å‹å: {self.train_model_name}")
        print(f"  â”œâ”€â”€ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"  â”œâ”€â”€ è¾“å‡ºæ¨¡å‹å: {self.output_model_name}")
        print(f"  â”œâ”€â”€ æ¨¡å‹æ–‡ä»¶åç¼€: {self.model_suffix}")
        print(f"  â””â”€â”€ ä¼˜åŒ–å™¨æ–‡ä»¶åç¼€: {self.optimizer_suffix}")
        
        # æ£€æŸ¥ç‚¹é…ç½®
        print("ğŸ’¾ æ£€æŸ¥ç‚¹é…ç½®:")
        print(f"  â”œâ”€â”€ æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡: {self.max_checkpoints}")
        print(f"  â””â”€â”€ ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹: {self.save_best_checkpoint}")
        
        # æ•°æ®é›†é…ç½®
        print("ğŸ“š æ•°æ®é›†é…ç½®:")
        print(f"  â”œâ”€â”€ è®­ç»ƒæ•°æ®è·¯å¾„: {self.train_data_path}")
        print(f"  â”œâ”€â”€ éªŒè¯æ•°æ®è·¯å¾„: {self.valid_data_path}")
        print(f"  â”œâ”€â”€ æµ‹è¯•æ•°æ®è·¯å¾„: {self.test_data_path}")
        print(f"  â”œâ”€â”€ åˆ†è¯å™¨è·¯å¾„: {self.tokenizer_path}")
        print(f"  â”œâ”€â”€ æ•°æ®åŠ è½½çº¿ç¨‹æ•°: {self.num_workers}")
        print(f"  â”œâ”€â”€ å›ºå®šå†…å­˜: {self.pin_memory}")
        print(f"  â””â”€â”€ æµå¼åŠ è½½: {self.yield_load}")
        
        # è®­ç»ƒå‚æ•°
        print("âš™ï¸ è®­ç»ƒå‚æ•°:")
        print(f"  â”œâ”€â”€ æ€»è®­ç»ƒè½®æ•°: {self.all_epochs}")
        print(f"  â”œâ”€â”€ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"  â”œâ”€â”€ åºåˆ—é•¿åº¦: {self.block_size}")
        print(f"  â”œâ”€â”€ æ¢¯åº¦ç´¯è®¡æ­¥æ•°: {self.accumulation_steps}")
        print(f"  â””â”€â”€ ä¿¡æ¯æ›´æ–°é—´éš”: {self.info_update_interval}")
        
        # ä¼˜åŒ–å™¨é…ç½®
        print("ğŸ“ˆ ä¼˜åŒ–å™¨é…ç½®:")
        print(f"  â”œâ”€â”€ ä¼˜åŒ–å™¨: {self.optimizer_name}")
        print(f"  â”œâ”€â”€ å­¦ä¹ ç‡: {self.learning_rate}")
        print(f"  â”œâ”€â”€ Betas: {self.betas}")
        print(f"  â”œâ”€â”€ Epsilon: {self.eps}")
        print(f"  â”œâ”€â”€ æƒé‡è¡°å‡: {self.weight_decay}")
        print(f"  â”œâ”€â”€ å­¦ä¹ ç‡è°ƒåº¦å™¨: {self.lr_scheduler}")
        print(f"  â”œâ”€â”€ é¢„çƒ­æ¯”ä¾‹: {self.pct_start}")
        print(f"  â”œâ”€â”€ æœ€å¤§å­¦ä¹ ç‡å€ç‡: {self.max_lr_rate}")
        print(f"  â”œâ”€â”€ åˆå§‹å­¦ä¹ ç‡å€ç‡: {self.div_factor}")
        print(f"  â””â”€â”€ é€€ç«ç­–ç•¥: {self.anneal_strategy}")
        
        # æ¨¡å‹æŠ€æœ¯å‚æ•°é…ç½®
        print("ğŸ¯ æ¨¡å‹æŠ€æœ¯å‚æ•°é…ç½®:")
        print(f"  â”œâ”€â”€ æ¢¯åº¦è£å‰ª: {self.grad_clip}")
        print(f"  â”œâ”€â”€ æ¢¯åº¦æ£€æŸ¥ç‚¹: {self.grad_checkpoint}")
        print(f"  â””â”€â”€ Dropout: {self.dropout}")
        
        # è¯„ä¼°é…ç½®
        print("ğŸ“Š è¯„ä¼°é…ç½®:")
        print(f"  â”œâ”€â”€ å›°æƒ‘åº¦è¯„ä¼°: {self.ppl_eval}")
        print(f"  â””â”€â”€ BLEU-4 è¯„ä¼°: {self.bleu_eval}")
        
        # å¯è§†åŒ–é…ç½®
        print("ğŸ“ˆ å¯è§†åŒ–é…ç½®:")
        print(f"  â”œâ”€â”€ TensorBoard: {self.tensorboard}")
        print(f"  â”œâ”€â”€ TensorBoard ç›®å½•: {self.tensorboard_dir}")
        print(f"  â””â”€â”€ TensorBoard æ—¥å¿—å: {self.writer_name}")
        
        # æ—¥å¿—é…ç½®
        print("ğŸ“ æ—¥å¿—é…ç½®:")
        print(f"  â”œâ”€â”€ æ—¥å¿—åç§°: {self.logger_name}")
        print(f"  â”œâ”€â”€ æ§åˆ¶å°æ—¥å¿—çº§åˆ«: {self.std_level}")
        print(f"  â”œâ”€â”€ æ–‡ä»¶æ—¥å¿—çº§åˆ«: {self.file_level}")
        print(f"  â”œâ”€â”€ æ§åˆ¶å°è¾“å‡º: {self.std_out}")
        print(f"  â”œâ”€â”€ ä¿å­˜åˆ°æ–‡ä»¶: {self.save_info}")
        print(f"  â””â”€â”€ æ—¥å¿—æ–‡ä»¶å: {self.file_name}")
        
        print("=" * 60)

    def update_step(self):
        pass

    def progress(self):
        """è¿›åº¦æ¡å¯è§†åŒ–è®­ç»ƒè¿›åº¦"""
        progress = Progress(
            TextColumn("[progress.description]{task.description}"),   # æ˜¾ç¤ºä»»åŠ¡çš„æè¿°ä¿¡æ¯
            BarColumn(),   # æ˜¾ç¤ºè¿›åº¦æ¡
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),   # è®¾ç½®æ ·å¼,ä¿ç•™ä¸‰ä½æ•°çš„æ•´æ•°ç™¾åˆ†æ¯”,å³å¯¹é½
            TimeRemainingColumn(),   # æ˜¾ç¤ºåŸºäºå½“å‰è¿›åº¦æ¨æµ‹ä¼°è®¡çš„å‰©ä½™æ—¶é—´
            TimeElapsedColumn(),   # æ˜¾ç¤ºè¿è¡Œæ—¶é—´
            TextColumn("[bold blue]{task.fields[show_info]}"),   # é¢å¤–ä¿¡æ¯
            refresh_per_second=1,  # æ¯1ç§’é’Ÿæ›´æ–°ä¸€æ¬¡
        )

        self.epoch_progress = progress.add_task(description='epoch: ', show_info='', total=self.all_epochs)
        # epochè¿›åº¦æ¡

        self.all_tsp = self.data_length * self.all_epochs //   \
            (self.batch_size * self.info_update_interval)
        self.tsp_progress = progress.add_task(description='steps: ', show_info='', total=self.all_tsp)
        # tspè¿›åº¦æ¡

        self.train_progress = progress   # å¯¹è±¡åŒ–è¿›åº¦æ¡
        self.train_progress.start()   # å¯åŠ¨è¿›åº¦æ¡

    def exit(self, signum, frame):
        """è¿›ç¨‹é€€å‡ºæ—¶è°ƒç”¨

        å‚æ•°:
        - signum (int): ä¿¡å·ç¼–å·
        - frame (frame): å½“å‰æ ˆå¸§
        """
        if self.train_signal:
            choice = input("ä½ æ­£åœ¨ä½¿ç”¨ `Ctrl+C` ä¸­æ–­è®­ç»ƒ, æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹? (y/n): ")

            if choice.lower() == 'y':
                now = datetime.now().strftime("%Y%m%d_%H%M%S")
                save_dir = os.path.join("exit_save", now+"_exit_ckpt")
                os.makedirs(save_dir, exist_ok=True)
                self.save_checkpoint(
                    dir=save_dir,
                    suffix=f"exit_{now}",
                )   # ä¿å­˜æ£€æŸ¥ç‚¹

                self._save_resume_page(
                    dir=save_dir,
                    suffix=f"exit_{now}",
                    now_time=now,
                )    # ä¿å­˜æ¢å¤é¡µé¢

                self.logger.info(f"é€€å‡ºè®­ç»ƒ, æ£€æŸ¥ç‚¹å·²ä¿å­˜è‡³ {save_dir}")
                self.train_progress.stop()
                sys.exit(0)

            else:
                self.logger.info("é€€å‡ºè®­ç»ƒ, ä¸ä¿å­˜æ£€æŸ¥ç‚¹")
                self.train_progress.stop()
                sys.exit(0)

        else:
            self.logger.info("æœªåœ¨è®­ç»ƒé˜¶æ®µ, é€€å‡ºè¿›ç¨‹")
            self.train_progress.stop()
            sys.exit(0)
