import json
import hashlib
import re
import random
from pathlib import Path
from typing import Any, Set, List, Tuple, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None
    print("âš ï¸ æœªå®‰è£… tqdmï¼Œè¿›åº¦æ¡ä¸å¯ç”¨ã€‚è¯·è¿è¡Œï¼špip install tqdm")

# ============ 1. æ¸…ç†å­—ç¬¦ä¸²ä¸­çš„ BiDi æ§åˆ¶å­—ç¬¦ ============
def clean_bidi_from_string(s: str) -> str:
    bidi_pattern = r'[\u202A-\u202E\u2066-\u2069\u200E\u200F]'
    return re.sub(bidi_pattern, '', s)

# ============ 2. é€’å½’æ¸…ç† JSON ä¸­æ‰€æœ‰å­—ç¬¦ä¸² + æ£€æµ‹æ˜¯å¦è¢«æ¸…æ´— ============
def clean_bidi_in_json_and_detect(obj: Any) -> Tuple[Any, bool]:
    if isinstance(obj, str):
        cleaned = clean_bidi_from_string(obj)
        was_cleaned = cleaned != obj
        return cleaned, was_cleaned
    elif isinstance(obj, dict):
        new_dict = {}
        was_cleaned = False
        for k, v in obj.items():
            cleaned_v, child_cleaned = clean_bidi_in_json_and_detect(v)
            new_dict[k] = cleaned_v
            if child_cleaned:
                was_cleaned = True
        return new_dict, was_cleaned
    elif isinstance(obj, list):
        new_list = []
        was_cleaned = False
        for item in obj:
            cleaned_item, child_cleaned = clean_bidi_in_json_and_detect(item)
            new_list.append(cleaned_item)
            if child_cleaned:
                was_cleaned = True
        return new_list, was_cleaned
    else:
        return obj, False

# ============ 3. å•æ–‡ä»¶å¤„ç†å‡½æ•°ï¼ˆWorkerï¼‰ ============
def process_single_file(file_path: Path, skip_invalid: bool = True):
    results = []
    total_lines_in_file = 0
    skipped_lines = 0
    cleaned_count = 0

    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            total_lines_in_file += 1
            stripped = line.rstrip('\r\n')
            if not stripped.strip():
                skipped_lines += 1
                continue

            try:
                obj = json.loads(stripped)
                cleaned_obj, was_cleaned = clean_bidi_in_json_and_detect(obj)
                if was_cleaned:
                    cleaned_count += 1
                results.append(cleaned_obj)
            except json.JSONDecodeError:
                if skip_invalid:
                    skipped_lines += 1
                    continue
                else:
                    raise

    return file_path.name, results, total_lines_in_file, skipped_lines, cleaned_count

# ============ 4. å…­åˆä¸€ç»ˆæå‡½æ•°ï¼ˆè®­ç»ƒé›†ä¸‰åˆ‡åˆ†ï¼‰ ============
def merge_clean_dedup_shuffle_split_multi_train(
    output_dir: str = ".",
    train_main_name: str = "train_main.jsonl",   # 60%
    train_aux_name: str = "train_aux.jsonl",     # 25%
    train_debug_name: str = "train_debug.jsonl", # 15%
    valid_name: str = "valid.jsonl",
    valid_size: int = 10000,
    train_ratios: tuple = (0.6, 0.25, 0.15),  # è®­ç»ƒé›†å†…éƒ¨åˆ†å‰²æ¯”ä¾‹
    dir_path: str = ".",
    file_pattern: str = "*.jsonl",
    hash_algo: str = 'md5',
    skip_invalid: bool = True,
    sort_keys_for_dedup: bool = True,
    max_workers: int = 8,
    seed: int = 42
):
    """
    ğŸš€ğŸš€ğŸš€ å…­åˆä¸€ï¼šåˆå¹¶ + æ¸…ç† BiDi + å»é‡ + æ‰“ä¹± + åˆ’åˆ†éªŒè¯é›† + è®­ç»ƒé›†ä¸‰åˆ‡åˆ†

    è¾“å‡ºæ–‡ä»¶ï¼š
      - {output_dir}/train_main.jsonl    â†’ 60% of train
      - {output_dir}/train_aux.jsonl     â†’ 25% of train
      - {output_dir}/train_debug.jsonl   â†’ 15% of train
      - {output_dir}/valid.jsonl         â†’ æœ€å valid_size æ¡

    æ–°å¢ï¼šè®­ç»ƒé›†å†…éƒ¨æŒ‰æ¯”ä¾‹åˆ‡å‰²
    """
    if max_workers is None:
        import os
        max_workers = min(32, (os.cpu_count() or 1) * 2)

    Path(output_dir).mkdir(parents=True, exist_ok=True)
    valid_path = Path(output_dir) / valid_name

    # 1ï¸âƒ£ æ‰¾æ–‡ä»¶
    input_files = list(Path(dir_path).glob(file_pattern))
    if not input_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• .jsonl æ–‡ä»¶")
        return

    input_files.sort(key=lambda x: x.name)
    print(f"ğŸ“ æ‰¾åˆ° {len(input_files)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} çº¿ç¨‹å¤„ç†")

    # 2ï¸âƒ£ é¢„ä¼°æ€»è¡Œæ•°ï¼ˆç”¨äºè¿›åº¦æ¡ï¼‰
    total_lines_estimate = 0
    for fp in input_files:
        try:
            with open(fp, 'r', encoding='utf-8') as f:
                total_lines_estimate += sum(1 for _ in f)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ç»Ÿè®¡ {fp.name} è¡Œæ•°: {e}")

    # 3ï¸âƒ£ åˆå§‹åŒ–ç»Ÿè®¡
    stats = {
        'total_raw_lines': 0,
        'total_skipped': 0,
        'total_cleaned_objects': 0,
        'total_after_clean': 0,
        'total_duplicates': 0,
        'final_unique': 0
    }

    # 4ï¸âƒ£ å¹¶è¡Œå¤„ç† + å»é‡
    seen_hashes: Set[str] = set()
    all_cleaned_items: List[Any] = []

    pbar = None
    if tqdm and total_lines_estimate > 0:
        pbar = tqdm(total=total_lines_estimate, desc="ğŸ§µ å¤šçº¿ç¨‹å¤„ç†ä¸­", unit="è¡Œ", smoothing=0.1)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_file = {
            executor.submit(process_single_file, fp, skip_invalid): fp.name
            for fp in input_files
        }

        for future in as_completed(future_to_file):
            filename = future_to_file[future]
            try:
                fname, results, raw_count, skipped, cleaned_cnt = future.result()

                stats['total_raw_lines'] += raw_count
                stats['total_skipped'] += skipped
                stats['total_cleaned_objects'] += cleaned_cnt
                stats['total_after_clean'] += len(results)

                for cleaned_obj in results:
                    if pbar:
                        pbar.update(1)

                    canonical_str = json.dumps(
                        cleaned_obj,
                        sort_keys=sort_keys_for_dedup,
                        ensure_ascii=False,
                        separators=(',', ':')
                    )
                    hasher = hashlib.new(hash_algo)
                    hasher.update(canonical_str.encode('utf-8'))
                    h = hasher.hexdigest()

                    if h in seen_hashes:
                        stats['total_duplicates'] += 1
                        continue

                    seen_hashes.add(h)
                    all_cleaned_items.append(cleaned_obj)

            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")

    if pbar:
        pbar.close()

    stats['final_unique'] = len(all_cleaned_items)
    total_before_shuffle = stats['final_unique']

    if total_before_shuffle == 0:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
        return

    # 5ï¸âƒ£ æ‰“ä¹±é¡ºåº
    print(f"ğŸ”€ æ­£åœ¨æ‰“ä¹± {total_before_shuffle} æ¡æ•°æ® (seed={seed})...")
    random.seed(seed)
    random.shuffle(all_cleaned_items)

    # 6ï¸âƒ£ åˆ’åˆ†éªŒè¯é›†
    if total_before_shuffle <= valid_size:
        print(f"âš ï¸ æ•°æ®æ€»é‡ {total_before_shuffle} <= éªŒè¯é›†å¤§å° {valid_size}ï¼Œå…¨éƒ¨ä½œä¸ºéªŒè¯é›†")
        train_items = []
        valid_items = all_cleaned_items
    else:
        split_point = total_before_shuffle - valid_size
        train_items = all_cleaned_items[:split_point]
        valid_items = all_cleaned_items[split_point:]

    # 7ï¸âƒ£ åˆ’åˆ†è®­ç»ƒå­é›†ï¼ˆæŒ‰æ¯”ä¾‹ï¼‰
    train_total = len(train_items)
    if train_total == 0:
        print("âš ï¸ è®­ç»ƒé›†ä¸ºç©ºï¼Œæ— æ³•åˆ‡åˆ†")
        train_main_items = []
        train_aux_items = []
        train_debug_items = []
    else:
        r1, r2, r3 = train_ratios
        # æ ¡éªŒæ¯”ä¾‹å’Œæ˜¯å¦ä¸º1
        if abs(r1 + r2 + r3 - 1.0) > 1e-5:
            print(f"âš ï¸ æ¯”ä¾‹å’Œä¸ä¸º1 ({r1}+{r2}+{r3}={r1+r2+r3})ï¼Œå·²è‡ªåŠ¨å½’ä¸€åŒ–")
            total_r = r1 + r2 + r3
            r1, r2, r3 = r1/total_r, r2/total_r, r3/total_r

        n1 = int(train_total * r1)
        n2 = int(train_total * r2)
        n3 = train_total - n1 - n2  # ç¡®ä¿æ€»æ•°ä¸å˜ï¼ˆé¿å…æµ®ç‚¹è¯¯å·®ï¼‰

        train_main_items = train_items[:n1]
        train_aux_items = train_items[n1:n1+n2]
        train_debug_items = train_items[n1+n2:]

    # 8ï¸âƒ£ å†™å…¥æ‰€æœ‰æ–‡ä»¶
    def write_jsonl(filepath: Path, items: List[Any], desc: str = ""):
        with open(filepath, 'w', encoding='utf-8') as f:
            for item in items:
                f.write(json.dumps(item, ensure_ascii=False, separators=(',', ':')) + '\n')
        count = len(items)
        if desc:
            print(f"   - {desc}: {count:,} æ¡ â†’ {filepath}")
        return count

    print("\nğŸ’¾ æ­£åœ¨å†™å…¥è¾“å‡ºæ–‡ä»¶...")
    write_jsonl(Path(output_dir) / train_main_name, train_main_items, "ä¸»è®­ç»ƒé›† (60%)")
    write_jsonl(Path(output_dir) / train_aux_name, train_aux_items, "è¾…åŠ©è®­ç»ƒé›† (25%)")
    write_jsonl(Path(output_dir) / train_debug_name, train_debug_items, "è°ƒè¯•è®­ç»ƒé›† (15%)")
    write_jsonl(valid_path, valid_items, "éªŒè¯é›†")

    # ğŸ‰ æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*70)
    print("âœ… å…­åˆä¸€å¤„ç†å®Œæˆï¼ç»ˆæç»Ÿè®¡æŠ¥å‘Š")
    print("="*70)
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶æ•°: {len(input_files)}")
    print(f"ğŸ§µ ä½¿ç”¨çº¿ç¨‹æ•°: {max_workers}")
    print()
    print("ğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"   - æœªæ¸…æ´—å‰æ€»è¡Œæ•°: {stats['total_raw_lines']:,}")
    print(f"   - è¢«è·³è¿‡çš„è¡Œæ•°: {stats['total_skipped']:,}")
    print(f"   - æœ‰æ•ˆ JSON å¯¹è±¡æ•°: {stats['total_after_clean']:,}")
    print()
    print("ğŸ§½ æ¸…æ´—ç»Ÿè®¡:")
    print(f"   - åŒ…å« BiDi æ§åˆ¶å­—ç¬¦çš„å¯¹è±¡æ•°: {stats['total_cleaned_objects']:,}")
    print()
    print("â™»ï¸  å»é‡ç»Ÿè®¡:")
    print(f"   - é‡å¤å¯¹è±¡æ•°: {stats['total_duplicates']:,}")
    print(f"   - å»é‡åå”¯ä¸€å¯¹è±¡æ•°: {stats['final_unique']:,}")
    print()
    print("âœ‚ï¸  æ•°æ®åˆ’åˆ†:")
    print(f"   - éªŒè¯é›†å¤§å°: {len(valid_items):,}")
    print(f"   - è®­ç»ƒé›†æ€»å¤§å°: {train_total:,}")
    print(f"     â”œâ”€ è®­ç»ƒé›† 256 (60%): {len(train_main_items):,}")
    print(f"     â”œâ”€ è®­ç»ƒé›† 512 (25%): {len(train_aux_items):,}")
    print(f"     â””â”€ è®­ç»ƒé›† 1024 (15%): {len(train_debug_items):,}")
    print("="*70)


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    merge_clean_dedup_shuffle_split_multi_train(
        output_dir="output",
        train_main_name="train_256.jsonl",
        train_aux_name="train_512.jsonl",
        train_debug_name="train_1024.jsonl",
        valid_name="valid.jsonl",
        valid_size=10000,
        train_ratios=(0.6, 0.25, 0.15),
        dir_path=".",
        file_pattern="*.jsonl",
        hash_algo="md5",
        skip_invalid=True,
        sort_keys_for_dedup=True,
        max_workers=8,
        seed=42
    )