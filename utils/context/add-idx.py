import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa
import os

def add_doc_id_streaming_by_row(
    input_path,
    output_path,
    doc_id_col_name='doc_id',
    chunk_size=1000000,
    start_id=0  # èµ·å§‹ IDï¼Œå¯è‡ªå®šä¹‰
):
    """
    æµå¼ä¸º Parquet æ–‡ä»¶æ¯ä¸€è¡Œæ·»åŠ é€’å¢ doc_idï¼ˆè¡Œå·ï¼‰ï¼Œä¸æ”¹å˜åŸå§‹åˆ—ã€‚
    é€‚ç”¨äºï¼šæ¯è¡Œæ˜¯ä¸€ä¸ªæ–‡æ¡£ï¼Œä½ æƒ³ç»™å®ƒä¸€ä¸ªå”¯ä¸€ IDã€‚
    
    å‚æ•°:
        input_path: è¾“å…¥ Parquet æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡º Parquet æ–‡ä»¶è·¯å¾„
        doc_id_col_name: doc_id åˆ—åï¼Œé»˜è®¤ 'doc_id'
        chunk_size: æ¯æ¬¡è¯»å–è¡Œæ•°
        start_id: doc_id èµ·å§‹å€¼
    """
    print(f"ğŸ“‚ å¼€å§‹æµå¼å¤„ç†: {input_path}")

    # è·å–å…ƒæ•°æ®
    meta = pq.read_metadata(input_path)
    total_rows = meta.num_rows
    schema_orig = meta.schema.to_arrow_schema()
    print(f"ğŸ“Š æ€»è¡Œæ•°: {total_rows}, åˆ†å—å¤§å°: {chunk_size}")

    # æ„å»ºæ–° schemaï¼šåœ¨æœ€å‰é¢æ’å…¥ doc_id åˆ—
    new_fields = [pa.field(doc_id_col_name, pa.int64())] + list(schema_orig)
    new_schema = pa.schema(new_fields)

    writer = None
    current_id = start_id
    processed_rows = 0

    try:
        parquet_file = pq.ParquetFile(input_path)
        
        for batch in parquet_file.iter_batches(batch_size=chunk_size):
            df_chunk = batch.to_pandas()

            # ğŸ‘‡ æ ¸å¿ƒï¼šæ·»åŠ é€’å¢ doc_idï¼ˆåŸºäºè¡Œå·ï¼‰
            chunk_size_actual = len(df_chunk)
            df_chunk.insert(0, doc_id_col_name, range(current_id, current_id + chunk_size_actual))

            # æ›´æ–°å½“å‰ ID
            current_id += chunk_size_actual

            # è½¬å› Arrow Tableï¼ˆä½¿ç”¨æ–° schemaï¼‰
            table = pa.Table.from_pandas(df_chunk, schema=new_schema)

            # åˆå§‹åŒ– writer
            if writer is None:
                writer = pq.ParquetWriter(output_path, new_schema)

            writer.write_table(table)
            processed_rows += chunk_size_actual
            print(f"âœ… å·²å¤„ç† {processed_rows} / {total_rows} è¡Œ (å½“å‰ doc_id åˆ° {current_id - 1})")

    finally:
        if writer:
            writer.close()
            print(f"ğŸ’¾ æœ€ç»ˆæ–‡ä»¶å·²ä¿å­˜: {output_path}")
        else:
            print("âš ï¸ æœªå†™å…¥ä»»ä½•æ•°æ®ã€‚")

    print("ğŸ‰ æµå¼æ·»åŠ  doc_id å®Œæˆï¼")

add_doc_id_streaming_by_row(
    input_path="data.parquet",    # åŸå§‹æ–‡ä»¶ï¼Œå“ªæ€•åªæœ‰ä¸€åˆ— 'text'
    output_path="data_with_doc_id.parquet",
    doc_id_col_name="doc_id",
    chunk_size=500000,
    start_id=0  # å¯é€‰ï¼šä» 0 å¼€å§‹ç¼–å·
)