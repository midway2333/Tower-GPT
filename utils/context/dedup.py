import pandas as pd
import pyarrow.parquet as pq
import pickle
from datasketch import MinHash, MinHashLSH
from tqdm import tqdm
import os
from multiprocessing import Pool, cpu_count
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import pyarrow as pa

# ========================
# é…ç½®åŒº
# ========================
PARQUET_FILE = "data_with_doc_id.parquet"
SIGNATURE_FILE = "minhash_signatures.pkl"
DEDUP_OUTPUT = "deduped_ids.txt"
BATCH_SIZE = 500000
NUM_PERM = 128
LSH_THRESHOLD = 0.85
N_JOBS = max(1, cpu_count() - 1)

# ========================
# MinHash è®¡ç®—å‡½æ•°ï¼ˆä¾›å¤šè¿›ç¨‹ä½¿ç”¨ï¼‰â€” å¿…é¡»åœ¨é¡¶å±‚å®šä¹‰
# ========================
def compute_minhash_for_row(args):
    doc_id, text = args
    m = MinHash(num_perm=NUM_PERM)
    if pd.isna(text):
        return doc_id, m
    for word in str(text).split():
        word = word.strip()
        if word:
            m.update(word.encode('utf-8'))
    return doc_id, m

# ========================
# ä¸»å‡½æ•°å°è£… â€”â€” é¿å…è¿›ç¨‹å†²çª
# ========================
def main():
    print("ğŸš€ Step 1: æµå¼è¯»å– Parquet å¹¶å¤šè¿›ç¨‹è®¡ç®— MinHash...")

    if os.path.exists(SIGNATURE_FILE):
        os.remove(SIGNATURE_FILE)

    parquet_file = pq.ParquetFile(PARQUET_FILE)
    total_rows = parquet_file.metadata.num_rows
    print(f"æ€»è¡Œæ•°: {total_rows}")
    print(f"ä½¿ç”¨ {N_JOBS} ä¸ªè¿›ç¨‹å¹¶è¡Œè®¡ç®—...")

    with open(SIGNATURE_FILE, "wb") as f_sig:
        for batch in tqdm(parquet_file.iter_batches(batch_size=BATCH_SIZE, columns=['doc_id', 'text']),
                          total=(total_rows + BATCH_SIZE - 1) // BATCH_SIZE,
                          desc="å¤„ç†æ‰¹æ¬¡"):
            df_batch = batch.to_pandas()
            tasks = df_batch[['doc_id', 'text']].values.tolist()

            # âš ï¸ å…³é”®ï¼šåœ¨æ¯ä¸ª batch å†…éƒ¨åˆ›å»º Poolï¼Œé¿å…å…¨å±€æ± å†²çª
            with Pool(processes=N_JOBS) as pool:
                results = list(tqdm(
                    pool.imap_unordered(compute_minhash_for_row, tasks, chunksize=1000),
                    total=len(tasks),
                    desc="å¹¶è¡Œè®¡ç®—MinHash",
                    leave=False
                ))

            for doc_id, m in results:
                pickle.dump((doc_id, m), f_sig)

    print("âœ… MinHash ç­¾åå·²å…¨éƒ¨å†™å…¥ç£ç›˜")

    # ========================
    # Step 2: LSH å»é‡
    # ========================
    print("ğŸ” Step 2: æ„å»º LSH å¹¶æ‰§è¡Œå»é‡...")

    lsh = MinHashLSH(threshold=LSH_THRESHOLD, num_perm=NUM_PERM)
    duplicates = set()
    kept = set()

    with open(SIGNATURE_FILE, "rb") as f_sig:
        pbar = tqdm(desc="å»é‡ä¸­")
        while True:
            try:
                doc_id, m = pickle.load(f_sig)
                pbar.update(1)

                candidates = lsh.query(m)
                if candidates:
                    duplicates.add(doc_id)
                else:
                    kept.add(doc_id)
                    lsh.insert(doc_id, m)

            except EOFError:
                break
        pbar.close()

    print(f"ğŸ“Œ æ€»æ–‡æ¡£æ•°: {len(kept) + len(duplicates)}")
    print(f"âœ… ä¿ç•™æ–‡æ¡£æ•°: {len(kept)}")
    print(f"ğŸ—‘ï¸  å»é‡æ–‡æ¡£æ•°: {len(duplicates)}")

    # ========================
    # Step 3: ä¿å­˜ç»“æœ
    # ========================
    print(f"ğŸ’¾ ä¿å­˜å»é‡åçš„ doc_id åˆ° {DEDUP_OUTPUT}...")

    with open(DEDUP_OUTPUT, "w", encoding="utf-8") as f_out:
        for doc_id in sorted(kept):
            f_out.write(f"{doc_id}\n")

    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")

# ========================
# âš ï¸ å…³é”®ï¼šé˜²æ­¢å¤šè¿›ç¨‹å†²çª
# ========================
if __name__ == '__main__':
    main()