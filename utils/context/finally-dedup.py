import pyarrow.parquet as pq
import pyarrow as pa
import pandas as pd
from tqdm import tqdm
import os

# ========================
# é…ç½®åŒºï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
# ========================
PARQUET_FILE = "data_with_doc_id.parquet"   # åŸå§‹è¾“å…¥æ–‡ä»¶
DEDUPED_IDS_FILE = "deduped_ids.txt"   # Step 3 è¾“å‡ºçš„ä¿ç•™ doc_id åˆ—è¡¨
OUTPUT_PARQUET = "deduped_data.parquet" # æœ€ç»ˆè¾“å‡º
BATCH_SIZE = 500000                     # æ¯æ‰¹å¤„ç†è¡Œæ•°ï¼ˆå†…å­˜å‹å¥½ï¼‰

# ========================
# Step 4: æµå¼è¯»å–åŸå§‹æ•°æ® + è¿‡æ»¤ä¿ç•™ doc_id â†’ æµå¼å†™å…¥æ–° Parquet
# ========================
print("ğŸ—ƒï¸  æ­£åœ¨æµå¼ç”Ÿæˆå»é‡åçš„å®Œæ•´æ•°æ®æ–‡ä»¶...")

# 1. åŠ è½½å»é‡åä¿ç•™çš„ doc_id é›†åˆï¼ˆä» deduped_ids.txtï¼‰
print(f"ğŸ“‚ è¯»å–ä¿ç•™ doc_id åˆ—è¡¨: {DEDUPED_IDS_FILE}")
with open(DEDUPED_IDS_FILE, "r", encoding="utf-8") as f:
    kept_ids = set(int(line.strip()) for line in f if line.strip())

print(f"ğŸ“Œ å…±åŠ è½½ {len(kept_ids)} ä¸ªä¿ç•™ doc_id")

# 2. æ‰“å¼€åŸå§‹ Parquet æ–‡ä»¶
parquet_file = pq.ParquetFile(PARQUET_FILE)
total_rows = parquet_file.metadata.num_rows
print(f"ğŸ“Š åŸå§‹æ•°æ®æ€»è¡Œæ•°: {total_rows}")

# 3. è·å– schemaï¼ˆä¿æŒåˆ—ç»“æ„ä¸€è‡´ï¼‰
schema = parquet_file.schema_arrow

# 4. åˆå§‹åŒ– ParquetWriter
writer = None
total_written = 0

try:
    for batch in tqdm(parquet_file.iter_batches(batch_size=BATCH_SIZE),
                      total=(total_rows + BATCH_SIZE - 1) // BATCH_SIZE,
                      desc="æµå¼å†™å…¥å»é‡æ•°æ®"):
        # è½¬ä¸º Pandas DataFrame
        df_batch = batch.to_pandas()

        # è¿‡æ»¤ï¼šåªä¿ç•™ kept_ids ä¸­çš„è¡Œ
        df_filtered = df_batch[df_batch['doc_id'].isin(kept_ids)]

        if len(df_filtered) == 0:
            continue

        # è½¬å› PyArrow Tableï¼ˆæŒ‡å®š schema é¿å…ç±»å‹é”™ä¹±ï¼‰
        table_batch = pa.Table.from_pandas(df_filtered, schema=schema, preserve_index=False)

        # é¦–æ¬¡å†™å…¥æ—¶åˆå§‹åŒ– writer
        if writer is None:
            writer = pq.ParquetWriter(
                OUTPUT_PARQUET,
                schema,
                compression='snappy',  # å¯é€‰ï¼š'gzip', 'zstd', None
                use_dictionary=True,
                write_statistics=True
            )

        writer.write_table(table_batch)
        total_written += len(df_filtered)

    print(f"âœ… æˆåŠŸå†™å…¥ {total_written} è¡Œåˆ° {OUTPUT_PARQUET}")
except Exception as e:
    print(f"âŒ å†™å…¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
finally:
    if writer:
        writer.close()
        print("ğŸ”’ Parquet æ–‡ä»¶å·²å®‰å…¨å…³é—­")

print("ğŸ‰ Step 4 å®Œæˆï¼")